{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpZaSh6SpcK4"
   },
   "source": [
    "## PFBS for RMM\n",
    "This code focuses on the core of the RMM, a joint PFBS/gradient descent algorithm on the residues. \n",
    "This part is then alternated with a barycentric pole relocation part to update the poles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7FH7w0iCd4R"
   },
   "source": [
    "This program implements in Python 3 :\n",
    "      - the simplest Vector Fitting algorithm (no complex conjugation for real case, no QR, no Relaxation, no othrogonal basis, etc.)\n",
    "      - adds a polynomial entire part to the rational fit, of arbitrary order poly_order\n",
    "      - Codes a PFBS algorithm for pole filtering\n",
    "\n",
    "NOTE : author is Pablo DUCRU, for any inquires please e-mail at  *** p_ducru@mit.edu ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tl0Uh7ZDCcC6"
   },
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import numpy as np\n",
    "from rmm import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"training_data/test_data_generation\"\n",
    "x,data_1, data_2, true_value_1, true_value_2, poles,residues = utils.load_data(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proximal operator for L1-L2 group regularization\n",
    "def prox_lam(A,lam):\n",
    "    return A*max(0,1-lam/np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tikhonov_gradient_Rn(Z,Y,rho,poles,R,mu,Delta_fun):\n",
    "    gradient = np.zeros([len(poles),len(Y[0])])\n",
    "    for n in range(len(poles)):\n",
    "        2*mu*gradient*R[n]/Delta_fun(Z,rho,poles[n])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SK_gradient_Rn(Z,Y,rho,poles,R,C,r):\n",
    "    gradient = np.zeros([len(poles),len(Y[0])])\n",
    "    for n in range(len(poles)):\n",
    "        for k in range(len(Z)):\n",
    "            Diff_k = utils.rational_function_at_z(z,poles,R,C) - utils.rational_function_at_z(z,poles,r,1)*Y[k]\n",
    "            gradient[n] += (rho[k]/np.conj(Z[k] - poles[n]))*Diff_k\n",
    "    return 2*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_Rn(Z,rho,poles,mu,Delta_fun):\n",
    "    temp = 0*1j\n",
    "    for k in range(len(Z)):\n",
    "        temp += rho[k]/(np.abs(Z[k]-poles[n])**2)\n",
    "    temp = 2*temp + 2*mu/Delta_fun(Z,rho,poles[n])\n",
    "    gamma_Rn = 1/temp\n",
    "    return gamma_Rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SK_gradient_C(Z,Y,rho,poles,R,C,r):\n",
    "    gradient = np.zeros([len(poles),len(Y[0])])\n",
    "    for n in range(len(poles)):\n",
    "        for k in range(len(Z)):\n",
    "            Diff_k = utils.rational_function_at_z(z,poles,R,C) - utils.rational_function_at_z(z,poles,r,1)*Y[k]\n",
    "            gradient[n] += rho[k]*Diff_k\n",
    "    return 2*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_C(Z,rho):\n",
    "    temp = 0*1j\n",
    "    for k in range(len(Z)):\n",
    "        temp += rho[k]\n",
    "    temp = 2*temp \n",
    "    gamma_C = 1/temp\n",
    "    return gamma_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SK_gradient_rn(Z,Y,rho,poles,R,C,r):\n",
    "    gradient = np.zeros([len(poles),len(Y[0])])\n",
    "    for n in range(len(poles)):\n",
    "        for k in range(len(Z)):\n",
    "            Diff_k = utils.rational_function_at_z(z,poles,R,C) - utils.rational_function_at_z(z,poles,r,1)*Y[k]\n",
    "            gradient[n] += (rho[k]/np.conj(Z[k] - poles[n]))*Y[k].conj().T@Diff_k\n",
    "    return -2*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_rn(Z,Y,rho,poles):\n",
    "    temp = 0*1j\n",
    "    temp_matrix = 0*1j*Y[0].conj().T@Y[0]\n",
    "    for k in range(len(Z)):\n",
    "        temp_matrix += rho[k]/(np.abs(Z[k]-poles[n])**2)*Y[k].conj().T@Y[k]\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(temp_matrix)\n",
    "    temp = 2*max(eigenvalues)\n",
    "    gamma_Rn = 1/temp\n",
    "    return gamma_Rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMM_PFBS_step(Z,Y,rho,poles,R,C,r,mu,lam,d_1,d_2):\n",
    "    R_next = R\n",
    "    C_next = C\n",
    "    r_next = r\n",
    "    \n",
    "    gamma_Rn = learning_rate_Rn(Z,rho,poles,mu,d_2)\n",
    "    gradient_Rn = SK_gradient_Rn(Z,Y,rho,poles,R,C,r) + Tikhonov_gradient_Rn(Z,Y,rho,poles,R,mu,d_2)\n",
    "    \n",
    "    gamma_C = learning_rate_C(Z,rho)\n",
    "    gradient_C = SK_gradient_C(Z,Y,rho,poles,R,C,r)\n",
    "    \n",
    "    gamma_rn = learning_rate_rn(Z,Y,rho,poles)\n",
    "    gradient_rn = SK_gradient_rn(Z,Y,rho,poles,R,C,r)\n",
    "    \n",
    "    \n",
    "    C_next = C - gamma_C*gradient_C\n",
    "    r_next = r - gamma_rn*gradient_rn\n",
    "    \n",
    "    for n,pole in enumerate(poles):\n",
    "        R_next[n] = prox_lam(R[n] - learning_rate_Rn(Z,rho,poles,mu,d_2)* gradient_Rn, lam/d_2(Z,rho,poles[n]))\n",
    "    \n",
    "    return R_next , C_next , r_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define path\n",
    "path = \"training_data/test_data_generation\"\n",
    "\n",
    "# Generate Data\n",
    "x, data_1, data_2, true_value_1, true_value_2, poles, residues = \\\n",
    "    rmm.generate_data.generate_data()\n",
    "\n",
    "# Save Data\n",
    "rmm.generate_data.save_data(path, x, data_1,\\\n",
    "                             data_2, true_value_1,true_value_2,poles,residues)\n",
    "\n",
    "# Plot and save to path\n",
    "rmm.generate_data.plot_data(x, data_1,data_2,true_value_1,\\\n",
    "                            true_value_2,poles,residues, path)\n",
    "\n",
    "# Test loading\n",
    "loaded_data = \\\n",
    "    rmm.generate_data.load_data(path)\n",
    "\n",
    "\n",
    "# %%\n",
    "importlib.reload(rmm)\n",
    "test = rmm.error_function.error(x,np.transpose(np.array([true_value_1,true_value_2])),[1/len(x)]*len(x),poles,residues,[0]*len(poles),1,0,0,rmm.error_function.delta_5,rmm.error_function.delta_5)\n",
    "test\n",
    "\n",
    "\n",
    "# %%\n",
    "Y = [np.real(rmm.utils.model(point, poles, residues[:,1],1)) for point in x]\n",
    "Y-true_value_2\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6GL17SqOQln"
   },
   "outputs": [],
   "source": [
    "## Import Python package for data management\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1k-5eSfJucXN"
   },
   "outputs": [],
   "source": [
    "## Importing Python packages for plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lphEFSoBo13y"
   },
   "source": [
    "The space of functions we are learning in is that of proper rational fraction of degree zero (poles + offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FICB4ZwD8j_N"
   },
   "outputs": [],
   "source": [
    "## Generic rational fractions with an offset and an entire part.\n",
    "# > The entire part is specified by the set of coefficients.\n",
    "# > If no coefficients are provided (the empty set), the rational function is build with a simple offset, the default value of which is zero.\n",
    "def rational_function(z, poles, residues, offset=0, poly_coeff=()):\n",
    "    if poly_coeff == ():\n",
    "        poly_order = 0\n",
    "        ## print(\"The rational_function function was not given an entire polynomial part to build.\")\n",
    "    else:\n",
    "        poly_order = poly_coeff.shape[0]\n",
    "    return  sum(residues[n]/(z-poles[n]) for n in range(poles.size)) + offset + sum(poly_coeff[n]*z**(n+1) for n in range(poly_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rational_function(1+1j, np.array([1, 1j]), np.array([[1, 2,3],[10,20,30]]), offset=0, poly_coeff=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpOXWYI6RVlV"
   },
   "source": [
    "## Training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJJCWgGTsnVq"
   },
   "source": [
    "#### Import real learning and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yh8BbMGSORYv"
   },
   "source": [
    "Read in cross sections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "mvxHgqh-RlEk",
    "outputId": "114a9622-f0c5-44ae-e1b5-abf2348b22e1"
   },
   "source": [
    "## retrieve labers strings data\n",
    "!wget https://www.dropbox.com/s/kfu0pjvrw2zky3h/temperature_cross_sections_Ce_140.csv?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "eZjaDbm7SlJk",
    "outputId": "8496895d-bff4-4763-c80f-a4f5995fa138"
   },
   "source": [
    "## read in the cvs cross section data\n",
    "cross_section_data = pd.read_csv('temperature_cross_sections_Ce_140.csv?dl=0') ## This dl=0 may change to dl=0.1,2,3.etc. if reading the Dropbox from another computer/during another session \n",
    "cross_section_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "1lFexxPpS-wp",
    "outputId": "55cc4718-ae72-4eb1-a6f0-e95610c73d9f"
   },
   "source": [
    "z_train = np.array(cross_section_data.Energy_eV, dtype=complex)\n",
    "z_train = np.sqrt(z_train[180:490]) ## in sqrt of energy space making smaller for testing\n",
    "z_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "CUF2js8DWiqP",
    "outputId": "7e118bc2-db0c-4b8f-c468-060b3cdd061a"
   },
   "source": [
    "Y_train = np.array(cross_section_data.iloc[:,1:9].copy(), dtype=complex)\n",
    "Y_train = Y_train[180:490,[0,4,7]] ## Y_train[200:475,[0,7]] \n",
    "Y_train = np.array([Y_train[n,:] for n in range(z_train.size)]) ##[z_train[n]*\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "colab_type": "code",
    "id": "q9UGCN2daMiN",
    "outputId": "b4668b33-28cb-4adb-bf91-d447347fb3d1"
   },
   "source": [
    "## Ploting training data\n",
    "fig_training_points = plt.figure()\n",
    "plt.plot(z_train, Y_train)\n",
    "plt.show()\n",
    "\n",
    "fig_training_points_log = plt.figure()\n",
    "plt.semilogy(z_train, Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iydldvDqs02d"
   },
   "source": [
    "#### Generate toy-problem training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TB1z5ipypo3s"
   },
   "source": [
    "Generate labeled data:\n",
    "\n",
    "We here only look at the case of vector residues, the matrix case with nuclear norm regularization will only necesitate an additional function to convert matrix to vector and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_true_poles = 10\n",
    "U = np.zeros([number_true_poles+2,number_true_poles+2])\n",
    "for i in range(number_true_poles+2):\n",
    "    for j in range(i,number_true_poles+2):\n",
    "        U[i,j] = np.random.rand()/2\n",
    "#print(U)\n",
    "U = U + np.transpose(U)\n",
    "#print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vp= np.sort(np.linalg.eig(U)[0][1:number_true_poles+2])\n",
    "plt.scatter(np.real(vp), np.imag(vp), s=80, facecolors='none', edgecolors='r')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_vp = np.zeros([vp.size-1]) \n",
    "for k in range(vp.size-1):\n",
    "    distances_vp[k] = vp[k+1] - vp[k]\n",
    "plt.scatter(np.real(distances_vp), np.imag(distances_vp), s=80, facecolors='none', edgecolors='r')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_poles = 100*distances_vp + 1j*(np.random.chisquare(1,number_true_poles)+np.random.chisquare(2,number_true_poles)+np.random.chisquare(10,number_true_poles))\n",
    "plt.scatter(np.real(true_poles), np.imag(true_poles), s=80, facecolors='none', edgecolors='r')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXuKgggXDjT8"
   },
   "outputs": [],
   "source": [
    "## True poles, generated at random\n",
    "number_true_poles = 10  ## >Physics: number of resonances of the scattering matrix\n",
    "\n",
    "\n",
    "\n",
    "true_poles = np.linspace(0, 100, number_true_poles) + np.array([ np.exp(1j*2*np.pi*np.random.rand()) for n in range(number_true_poles)])\n",
    "\n",
    "plt.scatter(np.real(true_poles), np.imag(true_poles), s=80, facecolors='none', edgecolors='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0653IrXz7-rw"
   },
   "outputs": [],
   "source": [
    "## True residues, generated at random\n",
    "dim_residues = 1 ## >Physics: size of the vectors being fitted, say your number of reaction channels (fission, scattering, γ-capture)\n",
    "true_residues = 3*np.array([ np.exp(1j*2*np.pi*np.random.rand(dim_residues)) for n in range(number_true_poles)])\n",
    "#Physical residues of different magnitude: \n",
    "#true_residues = 4*np.array([ ((true_poles[n]/number_true_poles)**0.5)*np.exp(1j*2*np.pi*np.random.rand(dim_residues)) for n in range(number_true_poles)])\n",
    "#true_residues = 6*np.array([ ((np.random.rand()))*np.exp(1j*2*np.pi*np.random.rand(dim_residues)) for n in range(number_true_poles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlz3DHwqwUMv"
   },
   "outputs": [],
   "source": [
    "## True offset\n",
    "true_offset = np.array(np.random.rand(dim_residues))-np.array(np.random.rand(dim_residues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXmOnhv6MRta"
   },
   "outputs": [],
   "source": [
    "## True polynomial coefficients\n",
    "true_poly_order = 0\n",
    "true_poly_coeff = 0.0*np.array([ (1/(n+1))*np.exp(1j*2*np.pi*np.random.rand(dim_residues)) for n in range(true_poly_order)])\n",
    "#true_poly_coeff = 0.001*np.array([ (1/(n+1))*np.exp(1j*2*np.pi*np.random.rand(dim_residues)) for n in range(true_poly_order)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zxorvqdu_wEt"
   },
   "source": [
    "Plot the true function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "MuTZDdtKH0Vn",
    "outputId": "1e75a109-6d82-4f21-e732-b81e6b29a59c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating the figure\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-2, 101, 0.5)\n",
    "Y = np.arange(-2, 2, 0.1)\n",
    "Z = np.zeros([X.size, Y.size] , dtype=complex)\n",
    "for i in range(X.size):\n",
    "  for j in range(Y.size):\n",
    "    Z[i,j] = rational_function(X[i]+1j*Y[j], true_poles, true_residues, true_offset, true_poly_coeff)[0]\n",
    "Zr = np.real(Z)\n",
    "Zi = np.imag(Z)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Zr.T, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(-10, 10)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=1, aspect=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "AcJiVDAUwFT_",
    "outputId": "7cf2638a-1045-49c2-fe63-d3649fe4af53"
   },
   "outputs": [],
   "source": [
    "## Ploting along the real axis\n",
    "\n",
    "x_grid = np.arange(-10, 110, 0.5)\n",
    "Z_x = np.zeros([x_grid.size, dim_residues] , dtype=complex)\n",
    "for i in range(x_grid.size):\n",
    "   Z_x[i] = rational_function( x_grid[i], true_poles, true_residues, true_offset, true_poly_coeff)\n",
    "Y_x = np.real(Z_x)\n",
    "plt.plot(x_grid , Y_x)\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(x_grid , Y_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "oQK6JWiW6qEO",
    "outputId": "2527fc27-140d-4ebe-9580-335d002314a7"
   },
   "outputs": [],
   "source": [
    "## Generating the training data (add statistical noise to test noisy data)\n",
    "number_train_points = 180\n",
    "## training mesh\n",
    "z_train = np.linspace(-20, 120, number_train_points)  ## + np.exp(1j*2*np.pi*np.random.rand(number_train_points)) for complex \n",
    "## corresponding training points\n",
    "Y_true = np.zeros([z_train.size, dim_residues] , dtype=complex)\n",
    "Y_train = np.zeros([z_train.size, dim_residues] , dtype=complex)\n",
    "μ =  0.0 ## mean deviation \n",
    "σ =  2.00 ## homeosckedastic case (full covariance for heterosckedastic case)\n",
    "for k in range(z_train.size):\n",
    "    Y_true[k] = rational_function(z_train[k], true_poles, true_residues, true_offset, true_poly_coeff) \n",
    "    ## Homoscedastic Normal: \n",
    "    Y_train[k] = rational_function(z_train[k], true_poles, true_residues, true_offset, true_poly_coeff)+ np.random.normal(μ, σ, dim_residues) ## Homoscedastic case\n",
    "    ## Homoscedastic Cauchy: \n",
    "    #Y_train[k] = rational_function(z_train[k], true_poles, true_residues, true_offset, true_poly_coeff)+ 1.5*np.ones(dim_residues)*np.random.standard_cauchy()\n",
    "    ## Linear heteroscedastic: Y_train[k] = rational_function(z_train[k], true_poles, true_residues, true_offset, true_poly_coeff)*( 1 + np.random.normal(μ, σ, dim_residues)) ## Linear Heteroscedastic case: \n",
    "    ## sqrt Homoscedastic: Y_train[k] = rational_function(z_train[k], true_poles, true_residues, true_offset, true_poly_coeff) + np.sqrt(np.linalg.norm(rational_function(z_train[k], true_poles, true_residues, true_offset, true_poly_coeff)))*np.random.normal(μ, σ, dim_residues) ## sqrt Heteroscedastic case: \n",
    "    \n",
    "    \n",
    "    ## Heteroscedastic case: y(k) + np.random.normal(0,sqrt(y(k)), dim_residues)\n",
    "noise_relative_error = np.linalg.norm(Y_true - Y_train)/(np.linalg.norm(Y_true))\n",
    "noise_relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1040
    },
    "colab_type": "code",
    "id": "OsMFuM5MGRj2",
    "outputId": "bae45ca2-72bd-4997-fc52-e28ca59743b0"
   },
   "outputs": [],
   "source": [
    "## Ploting the noisy training points upon the real data\n",
    "fig_training_points = plt.figure()\n",
    "#plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "plt.plot(z_train, Y_train, 'x k', label = 'Y_train')\n",
    "plt.show()\n",
    "\n",
    "fig_training_points = plt.figure()\n",
    "plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "plt.plot(z_train, Y_train, 'x k', label = 'Y_train')\n",
    "plt.show()\n",
    "\n",
    "fig_training_points_diff = plt.figure()\n",
    "plt.plot(z_train, Y_true - Y_train)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig_training_points_rel_diff = plt.figure()\n",
    "plt.plot(z_train, (Y_true - Y_train)/Y_true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Hzqb4Gyphl8"
   },
   "source": [
    "## Vector Fitting Algorithm\n",
    "\n",
    "\n",
    "\n",
    "##### The simplest barycentric SK iterations: no relaxation, no QR, no orthogonal basis, no real numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtItn5cJCaHu"
   },
   "source": [
    "#### VF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EcabUnz_DSsj"
   },
   "source": [
    ": Building functions that make up the building blocks of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJKoKzBx1qEy"
   },
   "outputs": [],
   "source": [
    "## build the square root of the rho_k weights for the LS system :: linear heteroscedastic case\n",
    "def build_square_weights_rho_k(z_train, Y_train):\n",
    "    number_train_points = z_train.size\n",
    "    sqrt_weights_rho = np.zeros([number_train_points], dtype=np.complex)\n",
    "    for k in range(number_train_points):\n",
    "        sqrt_weights_rho[k] =  1.0/(np.sqrt(number_train_points)) # CHANGE HERE FOR HETEROSKEDASTIC *(np.sqrt(np.linalg.norm(Y_train[k])))))  ## * (np.linalg.norm(Y_train[k])) ))) ## for linear heteroscedastic case, add: *np.linalg.norm(Y_train[k]))\n",
    "    return sqrt_weights_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3VqFWvaYHJP"
   },
   "outputs": [],
   "source": [
    "## define the Cauchy matrix\n",
    "def Cauchy_matrix(z,p):\n",
    "    C = np.zeros([z.size , p.size],  dtype=np.complex)\n",
    "    for k in range(z.size):\n",
    "        for j in range(p.size):\n",
    "            C[k,j] = 1/(z[k] - p[j])\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0oa-yjTy3w4"
   },
   "outputs": [],
   "source": [
    "## define the Vandermonde matrix without offset\n",
    "def Vandermonde_matrix(z,poly_order):\n",
    "    V = np.zeros([z.size , poly_order],  dtype=np.complex)\n",
    "    for k in range(z.size):\n",
    "        for n in range(poly_order):\n",
    "            V[k,n] = z[k]**(n+1)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8m3P_2-505n"
   },
   "outputs": [],
   "source": [
    "## Create the Y_vector for the LS problem :: Here, we weight it with the sqrt of the rho_k for the system\n",
    "def vectorize_Y_for_LS(z_train, Y_train, dim_residues):\n",
    "    number_train_points = Y_train.shape[0]\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train)\n",
    "    Y_LS_vector = np.zeros([dim_residues*number_train_points],  dtype=np.complex)\n",
    "    for d in range(dim_residues):\n",
    "        for k in range(number_train_points):\n",
    "            Y_LS_vector[d*number_train_points + k] =  sqrt_weights_rho[k]*Y_train[k,d]\n",
    "    return Y_LS_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2y8vBG1iCoaH"
   },
   "outputs": [],
   "source": [
    "## build the matrix of the barycentric LS system:\n",
    "def build_barycentric_LS_matrix(z_train, Y_train, learn_poles, poly_order=0):\n",
    "  dim_residues = Y_train[0].size\n",
    "  number_train_points = z_train.size\n",
    "  number_poles_learn = learn_poles.size\n",
    "  sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "  C = Cauchy_matrix(z_train, learn_poles)\n",
    "  V = Vandermonde_matrix(z_train,poly_order)\n",
    "  LS_matrix = np.zeros([dim_residues*number_train_points, dim_residues*(number_poles_learn+1+poly_order) + number_poles_learn],  dtype=np.complex)\n",
    "  for d in range(dim_residues):\n",
    "      for k in range(number_train_points):\n",
    "          for p in range(number_poles_learn):\n",
    "              LS_matrix[d*number_train_points + k, d*number_poles_learn + p] = sqrt_weights_rho[k]*C[k,p]\n",
    "              for n in range(poly_order):\n",
    "                  LS_matrix[d*number_train_points + k, dim_residues*number_poles_learn + d*poly_order + n] = sqrt_weights_rho[k]*V[k,n] \n",
    "              LS_matrix[d*number_train_points + k, dim_residues*(number_poles_learn+poly_order) + d] = sqrt_weights_rho[k]*1 \n",
    "              LS_matrix[d*number_train_points + k, dim_residues*(number_poles_learn+poly_order+1) + p] = -sqrt_weights_rho[k]*Y_train[k,d]*C[k,p]\n",
    "  return LS_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtijFXoua4Uh"
   },
   "outputs": [],
   "source": [
    "## build the matrix of the simple LS system:\n",
    "def build_LS_matrix(z_train, Y_train, learn_poles, poly_order=0):\n",
    "  dim_residues = Y_train[0].size\n",
    "  number_train_points = z_train.size\n",
    "  number_poles_learn = learn_poles.size\n",
    "  sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "  C = Cauchy_matrix(z_train, learn_poles)\n",
    "  V = Vandermonde_matrix(z_train,poly_order)\n",
    "  LS_matrix = np.zeros([dim_residues*number_train_points, dim_residues*(number_poles_learn+1+poly_order) ],  dtype=np.complex)\n",
    "  for d in range(dim_residues):\n",
    "      for k in range(number_train_points):\n",
    "          for p in range(number_poles_learn):\n",
    "              LS_matrix[d*number_train_points + k, d*number_poles_learn + p] = sqrt_weights_rho[k]*C[k,p]  \n",
    "              for n in range(poly_order):\n",
    "                  LS_matrix[d*number_train_points + k, dim_residues*number_poles_learn + d*poly_order + n] = sqrt_weights_rho[k]*V[k,n] \n",
    "              LS_matrix[d*number_train_points + k, dim_residues*(number_poles_learn+poly_order) + d] = sqrt_weights_rho[k]*1 \n",
    "  return LS_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zIVy-Y10sEQ"
   },
   "outputs": [],
   "source": [
    "## Function that vectorizes the residues and offset for the problem\n",
    "def build_LS_vector(residues, offset, poly_coeff=()):\n",
    "    if type(poly_coeff) == tuple:\n",
    "        poly_order = 0\n",
    "        #print(\"The build_LS_vector function was not given an entire polynomial part to build.\") \n",
    "    else:\n",
    "        poly_order = poly_coeff.shape[0]\n",
    "    number_poles = residues.shape[0]\n",
    "    dim_residues = residues.shape[1]\n",
    "    LS_vector    = np.zeros([dim_residues*(number_poles+poly_order+1)], dtype=np.complex)\n",
    "    for d in range(dim_residues):\n",
    "        for p in range(number_poles):\n",
    "            LS_vector[d*number_poles + p] = residues[p][d]\n",
    "        for n in range(poly_order):\n",
    "            LS_vector[dim_residues*number_poles + d*poly_order + n] = poly_coeff[n][d]\n",
    "    LS_vector[dim_residues*(number_poles+poly_order):LS_vector.size] = offset\n",
    "    return LS_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbJb2nad0xPx"
   },
   "outputs": [],
   "source": [
    "## Function that takes the vectorized solution and spits out the different elements\n",
    "def extract_from_LS_vector(LS_vector, number_poles, dim_residues , poly_order=0):\n",
    "    residues = np.zeros([number_poles, dim_residues], dtype=np.complex)\n",
    "    poly_coeff = np.zeros([poly_order, dim_residues], dtype=np.complex)\n",
    "    offset   = np.zeros([dim_residues], dtype = np.complex)\n",
    "    for d in range(dim_residues): \n",
    "        for p in range(number_poles):\n",
    "            residues[p][d] = LS_vector[d*number_poles + p]\n",
    "        for n in range(poly_order):\n",
    "            poly_coeff[n][d] = LS_vector[dim_residues*number_poles + d*poly_order + n]\n",
    "    offset = LS_vector[dim_residues*(number_poles+poly_order) : LS_vector.size]\n",
    "    if poly_order == 0:\n",
    "        return residues, offset\n",
    "    else:\n",
    "        return residues, poly_coeff, offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMB6A3WJFHF1"
   },
   "source": [
    "**The Vector Fitting algorithm: **\n",
    "\n",
    "The algorithm takes as input a training set {z_k, Y_k}, composed of training vectors {Y_k}, matched on a grid {z_k}.\n",
    "\n",
    "A standard number of VF iterations is 10, which usually suffices to converge the poles. \n",
    "\n",
    "Our implementation does not require the input of an initial guess. If you do not give one, the program will come up with a linearly spaced complex diagonal along the rectangle of the mesh of training points on the complex plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2dLKLHkFI8F"
   },
   "outputs": [],
   "source": [
    "def VF_algorithm(z_train, Y_train, number_VF_iteration, poly_order = 0 , *arguments ):\n",
    "    ## build the Y vector to solve for:\n",
    "    dim_residues = Y_train[0].size\n",
    "    number_train_points = z_train.size\n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    ## Initialize the poles\n",
    "    if arguments == ():\n",
    "        raise AssertionError(\"The VF_algorithm function must be given either a number of poles, or an array of initial poles guess\") \n",
    "    for arg in arguments:\n",
    "        if type(arg) == np.ndarray: ## was given an initial guess as argument\n",
    "            print(\" The VF_algorithm was provided an initial guess for the poles\")\n",
    "            learn_poles = arg\n",
    "            number_poles = learn_poles.size\n",
    "        elif type(arg) == int: ## was given a number of poles without any initial guess\n",
    "            print(\"The VF_algorithm was provided a number of poles to learn and is generating an initial guess\")\n",
    "            number_poles = arg\n",
    "            if (np.amax(np.imag(z_train)) - np.amin(np.imag(z_train))) == 0: ## only real training data\n",
    "                print(\"The training points are only along the real axis, and the initial guesses are generated accordingly with a shift\")\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train))+1/(10*(np.amax(np.real(z_train)) - np.amin(np.real(z_train)))),np.amax(np.real(z_train)) + 1/(10*(np.amin(np.real(z_train))-np.amax(np.real(z_train)))) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)),np.amax(np.imag(z_train)), number_poles)\n",
    "            elif (np.amax(np.real(z_train)) - np.amin(np.real(z_train)) ) == 0:\n",
    "                print(\"The training points are exactly along the imaginary axis, and the initial guesses are generated accordingly with a shift\")\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train)),np.amax(np.real(z_train)) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)) + 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))) ,np.amax(np.imag(z_train)) - 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))), number_poles)\n",
    "            else:\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train))+1/(10*(np.amax(np.real(z_train))-np.amin(np.real(z_train)))),np.amax(np.real(z_train)) - 1/(10*(np.amax(np.real(z_train)) - np.amin(np.real(z_train)))) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)) + 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))), np.amax(np.imag(z_train)) -  1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))) , number_poles)\n",
    "    ## POLE CONVERGENCE: Run the VF iterations \n",
    "    for i in range(number_VF_iteration):\n",
    "        ## build the barycentric L2 system\n",
    "        barycentric_LS_matrix = build_barycentric_LS_matrix(z_train, Y_train, learn_poles, poly_order)\n",
    "        ## solve the barycentric L2 system\n",
    "        barycentric_LS_vector , barycentric_LS_residual , barycentric_LS_rank , barycentric_LS_singular_values = np.linalg.lstsq(barycentric_LS_matrix, Y_LS_vector, poly_order)\n",
    "        ## extract the barycentric residues\n",
    "        barycentric_residues = barycentric_LS_vector[dim_residues*(learn_poles.size + poly_order +1):barycentric_LS_vector.size]\n",
    "        ## Build the matrix the spectrum of which will be the recolated poles\n",
    "        P = np.diag(learn_poles) - np.tensordot(barycentric_residues,np.ones([barycentric_residues.size], dtype=np.complex),0)\n",
    "        ## Solve the spectral problem & relocate poles\n",
    "        learn_poles , eigenvectors = np.linalg.eig(P)\n",
    "        ## convergence criteria\n",
    "    ## RESIDUES EXTRACTION: Solve the LS system\n",
    "    ## build the quadratic system\n",
    "    LS_matrix = build_LS_matrix(z_train, Y_train, learn_poles, poly_order)\n",
    "    ## solve the quadratic L2 system\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(LS_matrix, Y_LS_vector) ## np.linalg.solve(A.T.dot(A) + lamb * np.identity(n_col), A.T.dot(y)) For Tichonov\n",
    "    ## extract the residues, polynomial coefficients and offset\n",
    "    if poly_order == 0:\n",
    "        VF_poly_coeff= np.array([ 0.0 for n in range(poly_order)])\n",
    "        VF_residues , VF_offset = extract_from_LS_vector(LS_vector, learn_poles.size, dim_residues)\n",
    "        return learn_poles, VF_residues, VF_poly_coeff, VF_offset, LS_residual, barycentric_residues ## Artificially added an emplty set of VF_poly_coeff for homogeneity\n",
    "    else:\n",
    "        VF_residues , VF_poly_coeff, VF_offset = extract_from_LS_vector(LS_vector, learn_poles.size, dim_residues, poly_order)\n",
    "        return learn_poles, VF_residues, VF_poly_coeff, VF_offset, LS_residual, barycentric_residues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMk2S0oURs9D"
   },
   "source": [
    "#### Results benchmarking and analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVXBCitxjWoM"
   },
   "source": [
    "VF algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "_uW_TcLqH3ji",
    "outputId": "7911d5b4-9538-402d-898b-ddf4b25cf1b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VF_poles, VF_residues, VF_poly_coeff, VF_offset, VF_residual, barycentric_residues = VF_algorithm(z_train, Y_train, 30, 0, 20) ## add VF_poly_coeff when poly_order not zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJ975wEjzASe"
   },
   "outputs": [],
   "source": [
    "## VF results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_VF = np.zeros([z_train.size, dim_residues] , dtype=complex) ## VF solution\n",
    "for k in range(z_train.size):\n",
    "    Y_VF[k] = rational_function(z_train[k], VF_poles, VF_residues, VF_offset) # add when poly_order not zero : VF_poly_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edwEHNGTxN-P"
   },
   "source": [
    "Plotting Vector Fitting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_train_vs_VF = plt.figure()\n",
    "plt.plot(z_train, Y_train, 'x k', label='Y_train' )\n",
    "plt.plot(z_train, Y_VF, 'b', label='Y_VF')\n",
    "#plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "\n",
    "fig_train_vs_VF = plt.figure()\n",
    "plt.plot(z_train, Y_train, 'x k', label='Y_train' )\n",
    "plt.plot(z_train, Y_VF, 'b', label='Y_VF')\n",
    "plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "\n",
    "#plt.ylim(-20,20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('F(z)')\n",
    "plot_title = 'VF fit'\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.rcParams['axes.facecolor'] = '0.98'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOAXuUcfyq8l"
   },
   "source": [
    "## PFBS algorithm for Elastic Net group regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LK8EJI-2zJd5"
   },
   "source": [
    "We here implement a Proximal Forward-Backward Splitting (PFBS) algorithm for complex Frobenius group-LASSO regularization, with a Tichonov L2 regularization, solving the Elastic Net problem.\n",
    "\n",
    "Of critical importance is the fact that the Frobenius norm of a Matrix and its assciated vector are the same. This will not be true of the nuclear norm, where the vectors will have to be compacted back as matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Purpose\n",
    "The Goal of this standard approach is to show that it DOES NOT perform well on our problem (even though many people would think it does), whereas our Pole Filtering regularization DOES work very well (because we though about the nature of the feature space). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTIvzAVUNmg3"
   },
   "source": [
    "#### PFBS algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESLFzCV3yzZG"
   },
   "source": [
    "Proximal operator for the Frobenius norm under cartesian differentiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkmuaGY2y8vz"
   },
   "outputs": [],
   "source": [
    "## Proximal operator for L1-L2 group regularization\n",
    "def prox_λ(A,λ):\n",
    "    return A*max(0,1-λ/np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qAmM4a7boxm"
   },
   "source": [
    "Our PFBS iterations solve the Elastic net Regularization problem (default LASSO and Tichonov Regularizations are zero, yielding the LS problem).\n",
    "\n",
    "The convergence criteria are given by a relative change threshold, as well as a total amount of iterations. The default values are 2000 iterations, and 10e-8 relative step sizes stop iteration convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLpgMzQhbkjt"
   },
   "outputs": [],
   "source": [
    "def PFBS_Elastic_Net_algorithm(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size\n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ)\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMM_algorithm(z_train, Y_train, number_VF_iteration, poly_order = 0 , *arguments ):\n",
    "    ## build the Y vector to solve for:\n",
    "    dim_residues = Y_train[0].size\n",
    "    number_train_points = z_train.size\n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    ## Initialize the poles\n",
    "    if arguments == ():\n",
    "        raise AssertionError(\"The VF_algorithm function must be given either a number of poles, or an array of initial poles guess\") \n",
    "    for arg in arguments:\n",
    "        if type(arg) == np.ndarray: ## was given an initial guess as argument\n",
    "            print(\" The VF_algorithm was provided an initial guess for the poles\")\n",
    "            learn_poles = arg\n",
    "            number_poles = learn_poles.size\n",
    "        elif type(arg) == int: ## was given a number of poles without any initial guess\n",
    "            print(\"The VF_algorithm was provided a number of poles to learn and is generating an initial guess\")\n",
    "            number_poles = arg\n",
    "            if (np.amax(np.imag(z_train)) - np.amin(np.imag(z_train))) == 0: ## only real training data\n",
    "                print(\"The training points are only along the real axis, and the initial guesses are generated accordingly with a shift\")\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train))+1/(10*(np.amax(np.real(z_train)) - np.amin(np.real(z_train)))),np.amax(np.real(z_train)) + 1/(10*(np.amin(np.real(z_train))-np.amax(np.real(z_train)))) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)),np.amax(np.imag(z_train)), number_poles)\n",
    "            elif (np.amax(np.real(z_train)) - np.amin(np.real(z_train)) ) == 0:\n",
    "                print(\"The training points are exactly along the imaginary axis, and the initial guesses are generated accordingly with a shift\")\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train)),np.amax(np.real(z_train)) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)) + 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))) ,np.amax(np.imag(z_train)) - 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))), number_poles)\n",
    "            else:\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train))+1/(10*(np.amax(np.real(z_train))-np.amin(np.real(z_train)))),np.amax(np.real(z_train)) - 1/(10*(np.amax(np.real(z_train)) - np.amin(np.real(z_train)))) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)) + 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))), np.amax(np.imag(z_train)) -  1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))) , number_poles)\n",
    "    ## POLE CONVERGENCE: Run the VF iterations \n",
    "    for i in range(number_VF_iteration):\n",
    "        ## build the barycentric L2 system\n",
    "        barycentric_LS_matrix = build_barycentric_LS_matrix(z_train, Y_train, learn_poles, poly_order)\n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## solve the barycentric L2 system\n",
    "        barycentric_LS_vector , barycentric_LS_residual , barycentric_LS_rank , barycentric_LS_singular_values = np.linalg.lstsq(barycentric_LS_matrix, Y_LS_vector, poly_order)\n",
    "        ## extract the barycentric residues\n",
    "        barycentric_residues = barycentric_LS_vector[dim_residues*(learn_poles.size + poly_order +1):barycentric_LS_vector.size]\n",
    "        ## Build the matrix the spectrum of which will be the recolated poles\n",
    "        P = np.diag(learn_poles) - np.tensordot(barycentric_residues,np.ones([barycentric_residues.size], dtype=np.complex),0)\n",
    "        ## Solve the spectral problem & relocate poles\n",
    "        learn_poles , eigenvectors = np.linalg.eig(P)\n",
    "        ## convergence criteria\n",
    "    ## RESIDUES EXTRACTION: Solve the LS system\n",
    "    ## build the quadratic system\n",
    "    LS_matrix = build_LS_matrix(z_train, Y_train, learn_poles, poly_order)\n",
    "    ## solve the quadratic L2 system\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(LS_matrix, Y_LS_vector) ## np.linalg.solve(A.T.dot(A) + lamb * np.identity(n_col), A.T.dot(y)) For Tichonov\n",
    "    ## extract the residues, polynomial coefficients and offset\n",
    "    if poly_order == 0:\n",
    "        VF_poly_coeff= np.array([ 0.0 for n in range(poly_order)])\n",
    "        VF_residues , VF_offset = extract_from_LS_vector(LS_vector, learn_poles.size, dim_residues)\n",
    "        return learn_poles, VF_residues, VF_poly_coeff, VF_offset, LS_residual, barycentric_residues ## Artificially added an emplty set of VF_poly_coeff for homogeneity\n",
    "    else:\n",
    "        VF_residues , VF_poly_coeff, VF_offset = extract_from_LS_vector(LS_vector, learn_poles.size, dim_residues, poly_order)\n",
    "        return learn_poles, VF_residues, VF_poly_coeff, VF_offset, LS_residual, barycentric_residues\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def RMM_regularization(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size  \n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    γ_R_p = np.zeros( [number_poles] , dtype=complex )\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "    for p in range(poles.size):\n",
    "        γ_R_p_inv = 0\n",
    "        for k in range(z_train.size):\n",
    "            γ_R_p_inv += 2*(sqrt_weights_rho[k]/np.abs(z_train[k] - poles[p]) )**2\n",
    "        γ_R_p[p] = (1/γ_R_p_inv)**0.5\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/γ_R_p[p])\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O__W6aVJN3_O"
   },
   "source": [
    "#### Results of the PFBS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prF0fAL9ODIN"
   },
   "source": [
    "The PFBS algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "S3_d41O6U92V",
    "outputId": "f219b00b-f57e-4236-d6c5-19658d40592d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PFBS_Elastic_Net_residues, PFBS_Elastic_Net_offset , γ , Elastic_Net_Condition_number, PFBS_Elastic_Net_iter_num = PFBS_Elastic_Net_algorithm(z_train, Y_train, VF_poles , 0.02, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0fvL52rbT3s",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PFBS results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_PFBS = np.zeros([z_train.size, dim_residues] , dtype=complex) ## VF solution\n",
    "for k in range(z_train.size):\n",
    "    Y_PFBS[k] = rational_function(z_train[k], VF_poles, PFBS_Elastic_Net_residues, PFBS_Elastic_Net_offset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2690
    },
    "colab_type": "code",
    "id": "mQ8XIAZUb5UC",
    "outputId": "5ef05dbe-ba2e-41ef-e665-568ded62f5c4"
   },
   "outputs": [],
   "source": [
    "fig_train_vs_VF = plt.figure()\n",
    "plt.plot(z_train, Y_train, 'x k', label='Y_train' )\n",
    "plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "plt.plot(z_train, Y_VF, 'b', label='Y_VF')\n",
    "plt.plot(z_train, Y_PFBS, 'g', label='Y_PFBS')\n",
    "\n",
    "\n",
    "\n",
    "#plt.ylim(-10, 20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('F(z)')\n",
    "plot_title = 'PFBS fit'\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.rcParams['axes.facecolor'] = '0.98'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ol4e4lnqbHf4"
   },
   "source": [
    "## Pole Filtering Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWuh2iIbbBJO"
   },
   "source": [
    "We here define a pole-filtering regularization, as explained in the equations of article here:\n",
    "https://www.overleaf.com/read/vjrwkmvhdzdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LR_7d_YqfXSR"
   },
   "outputs": [],
   "source": [
    "## Proximal operator for L1-L2 group regularization\n",
    "def prox_λ(A,λ):\n",
    "    return A*max(0,1-λ/np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUTdE1LJaXg1"
   },
   "outputs": [],
   "source": [
    "def Pole_filtering_regularization(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size\n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    min_distances_z_p = np.zeros( [number_poles] , dtype=complex )\n",
    "    for p in range(poles.size):\n",
    "        min_distances_z_p[p] = np.abs(poles[p] - z_train[0])\n",
    "        for k in range(z_train.size):\n",
    "            if np.abs(poles[p] - z_train[k]) < min_distances_z_p[p] :\n",
    "                min_distances_z_p[p] = np.abs(poles[p] - z_train[k])\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/min_distances_z_p[p])\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pole_squared_filtering_regularization(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size\n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    min_distances_z_p = np.zeros( [number_poles] , dtype=complex )\n",
    "    for p in range(poles.size):\n",
    "        min_distances_z_p[p] = np.abs(poles[p] - z_train[0])\n",
    "        for k in range(z_train.size):\n",
    "            if np.abs(poles[p] - z_train[k]) < min_distances_z_p[p] :\n",
    "                min_distances_z_p[p] = np.abs(poles[p] - z_train[k])\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/(min_distances_z_p[p]**2))\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMM_regularization(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size  \n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    γ_R_p = np.zeros( [number_poles] , dtype=complex )\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "    for p in range(poles.size):\n",
    "        γ_R_p_inv = 0\n",
    "        for k in range(z_train.size):\n",
    "            γ_R_p_inv += 2*(sqrt_weights_rho[k]/np.abs(z_train[k] - poles[p]) )**2\n",
    "        γ_R_p[p] = (1/γ_R_p_inv)**0.5\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/γ_R_p[p])\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMM2_regularization(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size  \n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    γ_R_p = np.zeros( [number_poles] , dtype=complex )\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "    for p in range(poles.size):\n",
    "        γ_R_p_inv = 0\n",
    "        for k in range(z_train.size):\n",
    "            γ_R_p_inv += 2*(sqrt_weights_rho[k]/np.abs(z_train[k] - poles[p]) )**2\n",
    "        γ_R_p[p] = 1/γ_R_p_inv\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/γ_R_p[p])\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rational_regularization(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size  \n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    Δ_R_p = np.zeros( [number_poles] , dtype=complex )\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "    for p in range(poles.size):\n",
    "        Δ_R_p_inv = 0\n",
    "        for k in range(z_train.size):\n",
    "            Δ_R_p_inv += (sqrt_weights_rho[k]**2)/np.abs(z_train[k] - poles[p])\n",
    "        Δ_R_p[p] = 1/Δ_R_p_inv\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/Δ_R_p[p])\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rational2_regularization(z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8 ):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size  \n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    Δ_R_p = np.zeros( [number_poles] , dtype=complex )\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "    for p in range(poles.size):\n",
    "        Δ_R_p_inv = 0\n",
    "        for k in range(z_train.size):\n",
    "            Δ_R_p_inv += ((sqrt_weights_rho[k]**2)/np.abs(z_train[k] - poles[p]))**2\n",
    "        Δ_R_p[p] = 1/Δ_R_p_inv\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/Δ_R_p[p])\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nbFTl6EbPNY"
   },
   "source": [
    "#### Results of the Pole Filtering Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfUBHxbMbSI4"
   },
   "source": [
    "Pole Filtering trumps L1-L2 group regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.0069\n",
    "λ2 = 0.0026\n",
    "\n",
    "λRMM = 0.06\n",
    "λRMM2 = 0.09\n",
    "\n",
    "λRational = 0.15\n",
    "λRational2 = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mjy8UqxthZyN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pole filtering\n",
    "Pole_filtered_residues, Pole_filtered_offset , γ , Pole_filtered_Condition_number, Pole_filtered_iter_num = Pole_filtering_regularization(z_train, Y_train, VF_poles , λ, 0.0)\n",
    "## Pole filtered results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_Pole_filtered = np.zeros([z_train.size, dim_residues] , dtype=complex) \n",
    "for k in range(z_train.size):\n",
    "    Y_Pole_filtered[k] = rational_function(z_train[k], VF_poles, Pole_filtered_residues, Pole_filtered_offset )\n",
    "\n",
    "# RMM filtered residues \n",
    "RMM_filtered_residues, RMM_filtered_offset , γ_RMM , RMM_filtered_Condition_number, RMM_filtered_iter_num = RMM_regularization(z_train, Y_train, VF_poles , λRMM, 0.0)\n",
    "## RMM filtered results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_RMM_filtered = np.zeros([z_train.size, dim_residues] , dtype=complex) \n",
    "for k in range(z_train.size):\n",
    "    Y_RMM_filtered[k] = rational_function(z_train[k], VF_poles, RMM_filtered_residues, RMM_filtered_offset )\n",
    "    \n",
    "\n",
    "# Pole squared filtering \n",
    "Pole_squared_filtered_residues, Pole_squared_filtered_offset , γ_squared , Pole_squared_filtered_Condition_number, Pole_squared_filtered_iter_num = Pole_squared_filtering_regularization(z_train, Y_train, VF_poles , λ2, 0.0)\n",
    "## Pole squared filtered results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_Pole_squared_filtered = np.zeros([z_train.size, dim_residues] , dtype=complex) \n",
    "for k in range(z_train.size):\n",
    "    Y_Pole_squared_filtered[k] = rational_function(z_train[k], VF_poles, Pole_squared_filtered_residues, Pole_squared_filtered_offset )\n",
    "\n",
    "    \n",
    "    \n",
    "# RMM2 filtered residues \n",
    "RMM2_filtered_residues, RMM2_filtered_offset , γ_RMM2 , RMM2_filtered_Condition_number, RMM2_filtered_iter_num = RMM2_regularization(z_train, Y_train, VF_poles , λRMM2, 0.0)\n",
    "## RMM2 filtered results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_RMM2_filtered = np.zeros([z_train.size, dim_residues] , dtype=complex) \n",
    "for k in range(z_train.size):\n",
    "    Y_RMM2_filtered[k] = rational_function(z_train[k], VF_poles, RMM2_filtered_residues, RMM2_filtered_offset )\n",
    "    \n",
    "# Rational filtering \n",
    "Rational_filtered_residues, Rational_filtered_offset , γ_Rational , Rational_filtered_Condition_number, Rational_filtered_iter_num = Rational_regularization(z_train, Y_train, VF_poles , λRational, 0.0)\n",
    "## Rational filtered results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_Rational_filtered = np.zeros([z_train.size, dim_residues] , dtype=complex) \n",
    "for k in range(z_train.size):\n",
    "    Y_Rational_filtered[k] = rational_function(z_train[k], VF_poles, Rational_filtered_residues, Rational_filtered_offset )\n",
    "\n",
    "\n",
    "# Rational2 filtering \n",
    "Rational2_filtered_residues, Rational2_filtered_offset , γ_Rational2 , Rational2_filtered_Condition_number, Rational2_filtered_iter_num = Rational2_regularization(z_train, Y_train, VF_poles , λRational2, 0.0)\n",
    "## Rational filtered results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_Rational2_filtered = np.zeros([z_train.size, dim_residues] , dtype=complex) \n",
    "for k in range(z_train.size):\n",
    "    Y_Rational2_filtered[k] = rational_function(z_train[k], VF_poles, Rational2_filtered_residues, Rational2_filtered_offset )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4010
    },
    "colab_type": "code",
    "id": "VySpx-LWh0BF",
    "outputId": "2a34e033-7195-46e5-eeb0-bfab6268d548",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plotting PFBS results v/s VF results v/s the true model data (log-space data is in absolute value)\n",
    "plt.figure()\n",
    "plot_title = 'Different regularizations'\n",
    "plt.plot(z_train, Y_train, 'x k', label='Y_train' )\n",
    "plt.plot(z_train, Y_VF, 'b', label='Y_VF' )\n",
    "#plt.plot(z_train, Y_PFBS, 'g', label='Y_PFBS')\n",
    "plt.plot(z_train, Y_Pole_filtered, 'c', label='Y_PF')\n",
    "plt.plot(z_train, Y_Pole_squared_filtered, 'm', label='Y_PF2')\n",
    "plt.plot(z_train, Y_RMM_filtered, 'y', label='Y_RMM')\n",
    "plt.plot(z_train, Y_RMM2_filtered, 'g', label='Y_RMM2')\n",
    "plt.plot(z_train, Y_Rational_filtered, label='Y_Rational')\n",
    "plt.plot(z_train, Y_Rational2_filtered, label='Y_Rational2')\n",
    "plt.plot(z_train, Y_true, 'r', label='Y_true' )\n",
    "#plt.ylim(-10, 20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('F(z)')\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "pdf_title = \"%s.%s\"%(plot_title , 'pdf')\n",
    "plt.savefig(pdf_title)\n",
    "\n",
    "plt.figure()\n",
    "plot_title = 'Different regularizations (semilog)'\n",
    "plt.semilogy(z_train, np.abs(Y_train), 'x k', label='Y_train' )\n",
    "plt.semilogy(z_train, np.abs(Y_VF), 'b', label='Y_VF' )\n",
    "#plt.semilogy(z_train, np.abs(Y_PFBS), 'g', label='Y_PFBS')\n",
    "plt.semilogy(z_train, np.abs(Y_Pole_filtered), 'c', label='Y_PF')\n",
    "plt.semilogy(z_train, np.abs(Y_Pole_squared_filtered), 'm', label='Y_PF2')\n",
    "plt.semilogy(z_train, np.abs(Y_RMM_filtered), 'y', label='Y_RMM')\n",
    "plt.semilogy(z_train, np.abs(Y_RMM2_filtered), 'g',  label='Y_RMM2')\n",
    "plt.semilogy(z_train, np.abs(Y_Rational_filtered), label='Y_Rational')\n",
    "plt.semilogy(z_train, np.abs(Y_Rational2_filtered), label='Y_Rational2')\n",
    "plt.semilogy(z_train, np.abs(Y_true), 'r', label='Y_true' )\n",
    "#plt.ylim(-10, 20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('|F(z)| (log scale)')\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "pdf_title = \"%s.%s\"%(plot_title , 'pdf')\n",
    "plt.savefig(pdf_title)\n",
    "\n",
    "plt.figure()\n",
    "plot_title = 'Relative difference to true'\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_VF)/Y_true), 'b', label='Y_VF' )\n",
    "#plt.semilogy(z_train, np.abs((Y_true - Y_PFBS)/Y_true), 'g', label='Y_PFBS')\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_Pole_filtered)/Y_true), 'c', label='Y_PF')\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_Pole_squared_filtered)/Y_true), 'm', label='Y_PF2')\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_RMM_filtered)/Y_true), 'y', label='Y_RMM')\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_RMM2_filtered)/Y_true), 'g', label='Y_RMM2')\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_Rational_filtered)/Y_true), label='Y_Rational')\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_Rational2_filtered)/Y_true), label='Y_Rational2')\n",
    "#plt.ylim(-10, 20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('|Y_true(z_k) - F(z_k)|/|Y_true(z_k)|')\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "pdf_title = \"%s.%s\"%(plot_title , 'pdf')\n",
    "plt.savefig(pdf_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining (to be done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 3 things remaining at this stage: \n",
    "> • benchmark the VF alroghtm: in particular its ability to catch all the right poles if provided with (noisy) data. Because in the end we are supposing that VF would if given the right amount of poles. What if this is not the case?\n",
    "\n",
    "> • Test which pole filtering regularization works best.\n",
    "\n",
    "> • Test pole v/s residues regularization for final result with the right nuber of poles. \n",
    "\n",
    "> • It is likely that all the results depend on the form of the noise. Choose the right noise. \n",
    "\n",
    "> • Test RMM v/s brute force VF on time and performance. \n",
    "\n",
    "> • Implement and test the validity of the adaptive learning rates $\\gamma_n = \\frac{1}{2\\sum_k \\frac{\\rho_k}{|z_k - p_n|^2}}$\n",
    " \n",
    " > • Propose, implement, and test relationship between noise and lambda $\\lambda \\sim \\sigma$ with $\\sigma \\sim \\sum_k \\rho_k \\left\\| F(z_k) - Y_k \\right\\|_2^2$ \n",
    " \n",
    " > • Test of real-case problem: oxygen-16 (good example because ENDF does not give the resonance parameters, so we can only \"blindly\" fit the date, and observe wether we find what R-matrix theory finds. If these are neutral-particle chanels, the fundamental assumption that we are searching for rational functions is valid, and then we are indeed testing the poerformance of the Rational Matrix Machine on noisy date for a real case. If it finds the exact solution, we could claim we have a \"New Machine Learning Algorithm Learns Nuclear Physics\", which is sexy enought, certainly for ICML, but if the algorithm is very good, why not in Nature Communciations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection | Residues finding\n",
    "Comparing the performance of Tichonov regularization, residues dampening regularization (with squares?), and direct least-squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BJJCWgGTsnVq",
    "MMk2S0oURs9D",
    "nTIvzAVUNmg3",
    "O__W6aVJN3_O",
    "0nbFTl6EbPNY"
   ],
   "name": "Copy of Rational Matrix Machines.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
