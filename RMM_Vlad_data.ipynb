{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpZaSh6SpcK4"
   },
   "source": [
    "# RATIONAL MATRIX MACHINES\n",
    "\n",
    "## Vector Fitting algorithm with expanded polynomial basis\n",
    "\n",
    "## + Proximal Forward Backward Splitting algorithm for Pole-filtering regularization \n",
    "\n",
    "### a generalized L1-L2 group regularization for complex pole-residues feature space\n",
    "\n",
    ">> ######    Homoscedastic & Heteroscedastic cases without polynomial background\n",
    "\n",
    "##### a Python implementation by P. DUCRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7FH7w0iCd4R"
   },
   "source": [
    "This program implements in Python 3 :\n",
    "      - the simplest Vector Fitting algorithm (no complex conjugation for real case, no QR, no Relaxation, no othrogonal basis, etc.)\n",
    "      - adds a polynomial entire part to the rational fit, of arbitrary order poly_order\n",
    "      - Codes a PFBS algorithm for pole filtering\n",
    "\n",
    "NOTE : author is Pablo DUCRU, for any inquires please e-mail at  *** p_ducru@mit.edu ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tl0Uh7ZDCcC6"
   },
   "outputs": [],
   "source": [
    "## Import Python package for linear algebra\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6GL17SqOQln"
   },
   "outputs": [],
   "source": [
    "## Import Python package for data management\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1k-5eSfJucXN"
   },
   "outputs": [],
   "source": [
    "## Importing Python packages for plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lphEFSoBo13y"
   },
   "source": [
    "### Rational Functions Space\n",
    "The space of functions we are learning in is that of proper rational fraction of degree zero (poles + offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FICB4ZwD8j_N"
   },
   "outputs": [],
   "source": [
    "## Generic rational fractions with an offset and an entire part.\n",
    "# > The entire part is specified by the set of coefficients.\n",
    "# > If no coefficients are provided (the empty set), the rational function is build with a simple offset, the default value of which is zero.\n",
    "def rational_function(z, poles, residues, offset=0, poly_coeff=()):\n",
    "    if poly_coeff == ():\n",
    "        poly_order = 0\n",
    "        ## print(\"The rational_function function was not given an entire polynomial part to build.\")\n",
    "    else:\n",
    "        poly_order = poly_coeff.shape[0]\n",
    "    return  sum(residues[n]/(z-poles[n]) for n in range(poles.size)) + offset + sum(poly_coeff[n]*z**(n+1) for n in range(poly_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valling Vlad Module\n",
    "Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "octave-cli not found, please see README",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-37945cd22248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_value_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_value_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_data/run1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrue_value_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_value_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Rational Matrix Machines/Felipe Git/Rational-Matrix-Machines/rmm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pkg\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mpkgutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__path__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m    \u001b[0m_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m    \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Rational Matrix Machines/Felipe Git/Rational-Matrix-Machines/rmm/generate_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moct2py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOct2Py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/oct2py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0moctave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOct2Py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mOct2PyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/oct2py/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, timeout, oned_as, temp_dir, convert_to_float, backend)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_user_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_ptrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/oct2py/core.py\u001b[0m in \u001b[0;36mrestart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         self._engine = OctaveEngine(stdin_handler=self._handle_stdin,\n\u001b[0;32m--> 518\u001b[0;31m                                     logger=self.logger)\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Add local Octave scripts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/octave_kernel/kernel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, error_handler, stream_handler, line_handler, stdin_handler, plot_settings, inline_toolkit, cli_options, logger)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcli_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_toolkit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minline_toolkit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/octave_kernel/kernel.py\u001b[0m in \u001b[0;36m_get_executable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'octave-cli'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexecutable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'octave-cli not found, please see README'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexecutable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: octave-cli not found, please see README"
     ]
    }
   ],
   "source": [
    "from rmm import utils as utils\n",
    "x, data_1, data_2, true_value_1, true_value_2, poles, residues = utils.load_data(\"training_data/run1\")\n",
    "data_1 = data_1.view('float')\n",
    "true_value_1 = true_value_1.view('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ploting the noisy training points upon the real data\n",
    "fig_training_points = plt.figure()\n",
    "#plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "plt.plot(x, data_1, 'x k', label = 'Y_train')\n",
    "plt.show()\n",
    "\n",
    "fig_training_points = plt.figure()\n",
    "plt.plot(x, true_value_1, 'r', label='Y_true')\n",
    "plt.plot(x, data_1, 'x k', label = 'Y_train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = x\n",
    "Y_train = data_1\n",
    "Y_train = Y_train.reshape((len(Y_train),1))\n",
    "Y_true = true_value_1\n",
    "Y_true = Y_true.reshape((len(Y_true),1))\n",
    "true_poles = poles\n",
    "true_residues = residues[:,0].reshape((len(residues),1))\n",
    "number_true_poles = len(true_poles)\n",
    "number_train_points = len(z_train)\n",
    "true_offset = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1040
    },
    "colab_type": "code",
    "id": "OsMFuM5MGRj2",
    "outputId": "bae45ca2-72bd-4997-fc52-e28ca59743b0"
   },
   "outputs": [],
   "source": [
    "## Ploting the noisy training points upon the real data\n",
    "fig_training_points = plt.figure()\n",
    "#plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "plt.plot(z_train, Y_train, 'x k', label = 'Y_train')\n",
    "plt.show()\n",
    "\n",
    "fig_training_points = plt.figure()\n",
    "plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "plt.plot(z_train, Y_train, 'x k', label = 'Y_train')\n",
    "plt.show()\n",
    "\n",
    "fig_training_points_diff = plt.figure()\n",
    "plt.plot(z_train, Y_true - Y_train)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig_training_points_rel_diff = plt.figure()\n",
    "plt.plot(z_train, (Y_true - Y_train)/Y_true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJKoKzBx1qEy"
   },
   "outputs": [],
   "source": [
    "## build the square root of the rho_k weights for the LS system :: linear heteroscedastic case\n",
    "def build_square_weights_rho_k(z_train, Y_train = ()):\n",
    "    number_train_points = z_train.size\n",
    "    sqrt_weights_rho = np.zeros([number_train_points], dtype=np.complex)\n",
    "    for k in range(number_train_points):\n",
    "        sqrt_weights_rho[k] =  1.0/(np.sqrt(number_train_points)) # CHANGE HERE FOR HETEROSKEDASTIC *(np.sqrt(np.linalg.norm(Y_train[k])))))  ## * (np.linalg.norm(Y_train[k])) ))) ## for linear heteroscedastic case, add: *np.linalg.norm(Y_train[k]))\n",
    "    return sqrt_weights_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3VqFWvaYHJP"
   },
   "outputs": [],
   "source": [
    "## define the Cauchy matrix\n",
    "def Cauchy_matrix(z,p):\n",
    "    C = np.zeros([z.size , p.size],  dtype=np.complex)\n",
    "    for k in range(z.size):\n",
    "        for j in range(p.size):\n",
    "            C[k,j] = 1/(z[k] - p[j])\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0oa-yjTy3w4"
   },
   "outputs": [],
   "source": [
    "## define the Vandermonde matrix without offset\n",
    "def Vandermonde_matrix(z,poly_order):\n",
    "    V = np.zeros([z.size , poly_order],  dtype=np.complex)\n",
    "    for k in range(z.size):\n",
    "        for n in range(poly_order):\n",
    "            V[k,n] = z[k]**(n+1)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8m3P_2-505n"
   },
   "outputs": [],
   "source": [
    "## Create the Y_vector for the LS problem :: Here, we weight it with the sqrt of the rho_k for the system\n",
    "def vectorize_Y_for_LS(z_train, Y_train, dim_residues):\n",
    "    number_train_points = Y_train.shape[0]\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train)\n",
    "    Y_LS_vector = np.zeros([dim_residues*number_train_points],  dtype=np.complex)\n",
    "    for d in range(dim_residues):\n",
    "        for k in range(number_train_points):\n",
    "            Y_LS_vector[d*number_train_points + k] =  sqrt_weights_rho[k]*Y_train[k,d]\n",
    "    return Y_LS_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2y8vBG1iCoaH"
   },
   "outputs": [],
   "source": [
    "## build the matrix of the barycentric LS system:\n",
    "def build_barycentric_LS_matrix(z_train, Y_train, learn_poles, poly_order=0):\n",
    "    dim_residues = Y_train[0].size\n",
    "    number_train_points = z_train.size\n",
    "    number_poles_learn = learn_poles.size\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "    C = Cauchy_matrix(z_train, learn_poles)\n",
    "    V = Vandermonde_matrix(z_train,poly_order)\n",
    "    LS_matrix = np.zeros([dim_residues*number_train_points, dim_residues*(number_poles_learn+1+poly_order) + number_poles_learn],  dtype=np.complex)\n",
    "    for d in range(dim_residues):\n",
    "        for k in range(number_train_points):\n",
    "            for p in range(number_poles_learn):\n",
    "                LS_matrix[d*number_train_points + k, d*number_poles_learn + p] = sqrt_weights_rho[k]*C[k,p]\n",
    "                for n in range(poly_order):\n",
    "                    LS_matrix[d*number_train_points + k, dim_residues*number_poles_learn + d*poly_order + n] = sqrt_weights_rho[k]*V[k,n] \n",
    "                LS_matrix[d*number_train_points + k, dim_residues*(number_poles_learn+poly_order) + d] = sqrt_weights_rho[k]*1 \n",
    "                LS_matrix[d*number_train_points + k, dim_residues*(number_poles_learn+poly_order+1) + p] = -sqrt_weights_rho[k]*Y_train[k,d]*C[k,p]\n",
    "    return LS_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtijFXoua4Uh"
   },
   "outputs": [],
   "source": [
    "## build the matrix of the simple LS system:\n",
    "def build_LS_matrix(z_train, Y_train, learn_poles, poly_order=0):\n",
    "    dim_residues = Y_train[0].size\n",
    "    number_train_points = z_train.size\n",
    "    number_poles_learn = learn_poles.size\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train, Y_train) \n",
    "    C = Cauchy_matrix(z_train, learn_poles)\n",
    "    V = Vandermonde_matrix(z_train,poly_order)\n",
    "    LS_matrix = np.zeros([dim_residues*number_train_points, dim_residues*(number_poles_learn+1+poly_order) ],  dtype=np.complex)\n",
    "    for d in range(dim_residues):\n",
    "        for k in range(number_train_points):\n",
    "            for p in range(number_poles_learn):\n",
    "                LS_matrix[d*number_train_points + k, d*number_poles_learn + p] = sqrt_weights_rho[k]*C[k,p]  \n",
    "                for n in range(poly_order):\n",
    "                    LS_matrix[d*number_train_points + k, dim_residues*number_poles_learn + d*poly_order + n] = sqrt_weights_rho[k]*V[k,n] \n",
    "                LS_matrix[d*number_train_points + k, dim_residues*(number_poles_learn+poly_order) + d] = sqrt_weights_rho[k]*1 \n",
    "    return LS_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zIVy-Y10sEQ"
   },
   "outputs": [],
   "source": [
    "## Function that vectorizes the residues and offset for the problem\n",
    "def build_LS_vector(residues, offset, poly_coeff=()):\n",
    "    if type(poly_coeff) == tuple:\n",
    "        poly_order = 0\n",
    "        #print(\"The build_LS_vector function was not given an entire polynomial part to build.\") \n",
    "    else:\n",
    "        poly_order = poly_coeff.shape[0]\n",
    "    number_poles = residues.shape[0]\n",
    "    dim_residues = residues.shape[1]\n",
    "    LS_vector    = np.zeros([dim_residues*(number_poles+poly_order+1)], dtype=np.complex)\n",
    "    for d in range(dim_residues):\n",
    "        for p in range(number_poles):\n",
    "            LS_vector[d*number_poles + p] = residues[p][d]\n",
    "        for n in range(poly_order):\n",
    "            LS_vector[dim_residues*number_poles + d*poly_order + n] = poly_coeff[n][d]\n",
    "    LS_vector[dim_residues*(number_poles+poly_order):LS_vector.size] = offset\n",
    "    return LS_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbJb2nad0xPx"
   },
   "outputs": [],
   "source": [
    "## Function that takes the vectorized solution and spits out the different elements\n",
    "def extract_from_LS_vector(LS_vector, number_poles, dim_residues , poly_order=0):\n",
    "    residues = np.zeros([number_poles, dim_residues], dtype=np.complex)\n",
    "    poly_coeff = np.zeros([poly_order, dim_residues], dtype=np.complex)\n",
    "    offset   = np.zeros([dim_residues], dtype = np.complex)\n",
    "    for d in range(dim_residues): \n",
    "        for p in range(number_poles):\n",
    "            residues[p][d] = LS_vector[d*number_poles + p]\n",
    "        for n in range(poly_order):\n",
    "            poly_coeff[n][d] = LS_vector[dim_residues*number_poles + d*poly_order + n]\n",
    "    offset = LS_vector[dim_residues*(number_poles+poly_order) : LS_vector.size]\n",
    "    if poly_order == 0:\n",
    "        return residues, offset\n",
    "    else:\n",
    "        return residues, poly_coeff, offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMB6A3WJFHF1"
   },
   "source": [
    "**The Vector Fitting algorithm: **\n",
    "\n",
    "The algorithm takes as input a training set {z_k, Y_k}, composed of training vectors {Y_k}, matched on a grid {z_k}.\n",
    "\n",
    "A standard number of VF iterations is 10, which usually suffices to converge the poles. \n",
    "\n",
    "Our implementation does not require the input of an initial guess. If you do not give one, the program will come up with a linearly spaced complex diagonal along the rectangle of the mesh of training points on the complex plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2dLKLHkFI8F"
   },
   "outputs": [],
   "source": [
    "def VF_algorithm(z_train, Y_train, number_VF_iteration, poly_order = 0 , *arguments ):\n",
    "    ## build the Y vector to solve for:\n",
    "    dim_residues = Y_train[0].size\n",
    "    number_train_points = z_train.size\n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    ## Initialize the poles\n",
    "    if arguments == ():\n",
    "        raise AssertionError(\"The VF_algorithm function must be given either a number of poles, or an array of initial poles guess\") \n",
    "    for arg in arguments:\n",
    "        if type(arg) == np.ndarray: ## was given an initial guess as argument\n",
    "            print(\" The VF_algorithm was provided an initial guess for the poles\")\n",
    "            learn_poles = arg\n",
    "            number_poles = learn_poles.size\n",
    "        elif type(arg) == int: ## was given a number of poles without any initial guess\n",
    "            print(\"The VF_algorithm was provided a number of poles to learn and is generating an initial guess\")\n",
    "            number_poles = arg\n",
    "            if (np.amax(np.imag(z_train)) - np.amin(np.imag(z_train))) == 0: ## only real training data\n",
    "                print(\"The training points are only along the real axis, and the initial guesses are generated accordingly with a shift\")\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train))+1/(10*(np.amax(np.real(z_train)) - np.amin(np.real(z_train)))),np.amax(np.real(z_train)) + 1/(10*(np.amin(np.real(z_train))-np.amax(np.real(z_train)))) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)),np.amax(np.imag(z_train)), number_poles)\n",
    "            elif (np.amax(np.real(z_train)) - np.amin(np.real(z_train)) ) == 0:\n",
    "                print(\"The training points are exactly along the imaginary axis, and the initial guesses are generated accordingly with a shift\")\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train)),np.amax(np.real(z_train)) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)) + 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))) ,np.amax(np.imag(z_train)) - 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))), number_poles)\n",
    "            else:\n",
    "                learn_poles = np.linspace(np.amin(np.real(z_train))+1/(10*(np.amax(np.real(z_train))-np.amin(np.real(z_train)))),np.amax(np.real(z_train)) - 1/(10*(np.amax(np.real(z_train)) - np.amin(np.real(z_train)))) , number_poles) + 1j*np.linspace(np.amin(np.imag(z_train)) + 1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))), np.amax(np.imag(z_train)) -  1/(10*(np.amax(np.imag(z_train)) - np.amin(np.imag(z_train)))) , number_poles)\n",
    "    ## POLE CONVERGENCE: Run the VF iterations \n",
    "    for i in range(number_VF_iteration):\n",
    "        ## build the barycentric L2 system\n",
    "        barycentric_LS_matrix = build_barycentric_LS_matrix(z_train, Y_train, learn_poles, poly_order)\n",
    "        ## solve the barycentric L2 system\n",
    "        barycentric_LS_vector , barycentric_LS_residual , barycentric_LS_rank , barycentric_LS_singular_values = np.linalg.lstsq(barycentric_LS_matrix, Y_LS_vector, poly_order)\n",
    "        ## extract the barycentric residues\n",
    "        barycentric_residues = barycentric_LS_vector[dim_residues*(learn_poles.size + poly_order +1):barycentric_LS_vector.size]\n",
    "        ## Build the matrix the spectrum of which will be the recolated poles\n",
    "        P = np.diag(learn_poles) - np.tensordot(barycentric_residues,np.ones([barycentric_residues.size], dtype=np.complex),0)\n",
    "        ## Solve the spectral problem & relocate poles\n",
    "        learn_poles , eigenvectors = np.linalg.eig(P)\n",
    "        ## convergence criteria\n",
    "    ## RESIDUES EXTRACTION: Solve the LS system\n",
    "    ## build the quadratic system\n",
    "    LS_matrix = build_LS_matrix(z_train, Y_train, learn_poles, poly_order)\n",
    "    ## solve the quadratic L2 system\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(LS_matrix, Y_LS_vector) ## np.linalg.solve(A.T.dot(A) + lamb * np.identity(n_col), A.T.dot(y)) For Tichonov\n",
    "    ## extract the residues, polynomial coefficients and offset\n",
    "    if poly_order == 0:\n",
    "        VF_poly_coeff= np.array([ 0.0 for n in range(poly_order)])\n",
    "        VF_residues , VF_offset = extract_from_LS_vector(LS_vector, learn_poles.size, dim_residues)\n",
    "        return learn_poles, VF_residues, VF_poly_coeff, VF_offset, LS_residual, barycentric_residues ## Artificially added an emplty set of VF_poly_coeff for homogeneity\n",
    "    else:\n",
    "        VF_residues , VF_poly_coeff, VF_offset = extract_from_LS_vector(LS_vector, learn_poles.size, dim_residues, poly_order)\n",
    "        return learn_poles, VF_residues, VF_poly_coeff, VF_offset, LS_residual, barycentric_residues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMk2S0oURs9D"
   },
   "source": [
    "#### Results benchmarking and analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVXBCitxjWoM"
   },
   "source": [
    "VF algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "_uW_TcLqH3ji",
    "outputId": "7911d5b4-9538-402d-898b-ddf4b25cf1b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VF_poles, VF_residues, VF_poly_coeff, VF_offset, VF_residual, barycentric_residues = VF_algorithm(z_train, Y_train, 30, 0, 25) ## add VF_poly_coeff when poly_order not zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that measures how well did the VF algorithm the true poles. \n",
    "def VF_finds_true_poles_accuracy(true_poles, VF_poles):\n",
    "    performance = 0\n",
    "    for pole in true_poles:\n",
    "        performance += min([np.abs(pole-pole_vf) for pole_vf in VF_poles])\n",
    "    return performance/len(true_poles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = VF_finds_true_poles_accuracy(true_poles, VF_poles)\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "void_array = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "void_array == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_test= np.array([1,2.91, 2, 3, 0])\n",
    "arr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr = arr_test[ [element == 10 for element in arr_test] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr.size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min( np.abs( (2.91 - pole)/2.91) for pole in arr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_pole_index = np.argmin([np.abs( (2.91 - pole)/2.91) for pole in arr_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.abs( (2.91 - pole)/2.91) for pole in arr_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_pole_index = np.argmin([np.abs( (2.91 - pole)/2.91) for pole in arr_test])\n",
    "closest_pole = arr_test[closest_pole_index ]\n",
    "closest_pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that measures the rate of missed poles: 1 = all poles missed 100% (i.e. either no poles to fit this true pole with, or fitted with zero a non-zero pole)\n",
    "def poles_missing_rate(true_poles, fit_poles):\n",
    "    miss = 0\n",
    "    remaining_poles = fit_poles\n",
    "    for true_pole in true_poles:\n",
    "        if remaining_poles.size > 0:\n",
    "            # find closest fit pole\n",
    "            closest_pole = remaining_poles[np.argmin([np.abs( (true_pole - pole)/true_pole ) for pole in remaining_poles ])]\n",
    "            # calculate how close (relatively) is the pole to the true poles\n",
    "            miss += np.abs( (true_pole - closest_pole)/true_pole ) \n",
    "            # remove this pole the the remaining poles\n",
    "            remaining_poles = remaining_poles[ [element != closest_pole for element in remaining_poles] ]\n",
    "        elif remaining_poles.size == 0: #not enought fiting poles for the number of true poles\n",
    "            miss += 1\n",
    "    return miss/len(true_poles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that measures the rate of missed poles: 1 = all poles missed 100% (i.e. either no poles to fit this true pole with, or fitted with zero a non-zero pole)\n",
    "def poles_overfitting_rate(true_poles, fit_poles):\n",
    "    overfit = 0\n",
    "    remaining_poles = true_poles\n",
    "    for fit_pole in fit_poles:\n",
    "        if remaining_poles.size > 0:\n",
    "            # find closest true pole\n",
    "            closest_pole = remaining_poles[np.argmin([np.abs( (fit_pole - pole)/pole ) for pole in remaining_poles ])]\n",
    "            # calculate how close (relatively) is the fit pole to the true poles\n",
    "            overfit += np.abs( (fit_pole - closest_pole)/closest_pole ) \n",
    "            # remove this pole the the remaining poles\n",
    "            remaining_poles = remaining_poles[ [element != closest_pole for element in remaining_poles] ]\n",
    "        elif remaining_poles.size == 0: #not enought fiting poles for the number of true poles\n",
    "            overfit += 1\n",
    "    return overfit/len(fit_poles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that measures the rate of finding all the poles, no more, no less (it gives 1 (100%) if all the poles are found exactly, and zero if all the poles are missed or all the poles are overfitted 100%)\n",
    "def poles_finding_rate(true_poles, fit_poles):\n",
    "    overfitting_rate = poles_overfitting_rate(true_poles, fit_poles)\n",
    "    missing_rate = poles_missing_rate(true_poles, fit_poles)\n",
    "    return 1 - max(missing_rate , overfitting_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_poles = np.array([1 ,2, 3, 4])\n",
    "fit_poles = np.array([1.1, 2.2, 3.3, 4.4] )\n",
    "print('poles_finding_rate = ', poles_finding_rate(true_poles, fit_poles) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that measures the rate of overfitting poles: 1 = all poles overfitted 100% (i.e. either no true poles corresponding to this pole, or fitted with zero a non-zero pole)\n",
    "def poles_missing_rate(true_poles, fit_poles):\n",
    "    overfit = 0\n",
    "    remaining_poles = true_poles\n",
    "    for fit_pole in fit_poles:\n",
    "        if remaining_poles.size > 0:\n",
    "            overfit += min( np.abs( (fit_pole - pole)/pole ) for pole in remaining_poles)\n",
    "        elif remaining_poles.size == 0: #not enought fiting poles for the number of true poles\n",
    "            miss += 1\n",
    "    return miss/len(true_poles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJ975wEjzASe"
   },
   "outputs": [],
   "source": [
    "## VF results\n",
    "z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "dim_residues = Y_train[0].size\n",
    "Y_VF = np.zeros([z_train.size, dim_residues] , dtype=complex) ## VF solution\n",
    "for k in range(z_train.size):\n",
    "    Y_VF[k] = rational_function(z_train[k], VF_poles, VF_residues, VF_offset) # add when poly_order not zero : VF_poly_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edwEHNGTxN-P"
   },
   "source": [
    "Plotting Vector Fitting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_train_vs_VF = plt.figure()\n",
    "plt.plot(z_train, Y_train, 'x k', label='Y_train' )\n",
    "plt.plot(z_train, Y_VF, 'b', label='Y_VF')\n",
    "#plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "\n",
    "fig_train_vs_VF = plt.figure()\n",
    "plt.plot(z_train, Y_train, 'x k', label='Y_train' )\n",
    "plt.plot(z_train, Y_VF, 'b', label='Y_VF')\n",
    "plt.plot(z_train, Y_true, 'r', label='Y_true')\n",
    "\n",
    "#plt.ylim(-20,20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('F(z)')\n",
    "plot_title = 'VF fit'\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.rcParams['axes.facecolor'] = '0.98'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VF_poles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ol4e4lnqbHf4"
   },
   "source": [
    "## RMM Regularizations for different Delta options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWuh2iIbbBJO"
   },
   "source": [
    "We here define a pole-filtering regularization, as explained in the equations of article here:\n",
    "https://www.overleaf.com/read/vjrwkmvhdzdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LR_7d_YqfXSR"
   },
   "outputs": [],
   "source": [
    "## Proximal operator for L1-L2 group regularization\n",
    "def prox_λ(A,λ):\n",
    "    return A*max(0,1-λ/np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_min_distance(z_train, poles, Y_train = ()):\n",
    "    min_distances_z_p = np.zeros(len(poles), dtype=complex )\n",
    "    for p in range(poles.size):\n",
    "        min_distances_z_p[p] = np.abs(poles[p] - z_train[0])\n",
    "        for k in range(z_train.size):\n",
    "            if np.abs(poles[p] - z_train[k]) < min_distances_z_p[p] :\n",
    "                min_distances_z_p[p] = np.abs(poles[p] - z_train[k])\n",
    "    return min_distances_z_p\n",
    "\n",
    "def delta_min_distance_sq(z_train,poles, Y_train = ()):\n",
    "    value = delta_min_distance(z_train,poles)\n",
    "    return value**2\n",
    "\n",
    "def delta_learning_rate(z_train, poles, Y_train=()):\n",
    "    Delta = np.zeros(len(poles) , dtype=complex )\n",
    "    sqrt_weights_rho = build_square_weights_rho_k(z_train) \n",
    "    for p in range(poles.size):\n",
    "        gamma_R_p_inv = 0\n",
    "        for k in range(z_train.size):\n",
    "            gamma_R_p_inv += 2*(sqrt_weights_rho[k]/np.abs(z_train[k] - poles[p]) )**2\n",
    "        Delta[p] = (1/gamma_R_p_inv)**0.5\n",
    "    return Delta \n",
    "\n",
    "def delta_learning_rate_sq(z_train, poles, Y_train=()):\n",
    "    value = delta_learning_rate(z_train, poles, Y_train)\n",
    "    return value**2\n",
    "\n",
    "def delta_average_pole(z_train, poles, Y_train = ()):\n",
    "    Delta = np.zeros(len(poles) , dtype=complex )\n",
    "    rho = build_square_weights_rho_k(z_train)**2 \n",
    "    for p in range(poles.size):\n",
    "        temp = 0\n",
    "        for k in range(z_train.size):\n",
    "            temp += rho[k]/np.abs(z_train[k] - poles[p])\n",
    "        Delta[p] = 1/temp\n",
    "    return Delta\n",
    "\n",
    "def delta_average_sq_pole(z_train, poles, Y_train = ()):\n",
    "    Delta = np.zeros(len(poles) , dtype=complex )\n",
    "    rho = build_square_weights_rho_k(z_train)**2 \n",
    "    for p in range(poles.size):\n",
    "        temp = 0\n",
    "        for k in range(z_train.size):\n",
    "            temp += (rho[k]/np.abs(z_train[k] - poles[p]))**2\n",
    "        Delta[p] = 1/temp\n",
    "    return Delta \n",
    "\n",
    "def delta_average_pole_sq(z_train, poles, Y_train = ()):\n",
    "    value = delta_average_pole(z_train, poles, Y_train)\n",
    "    return value**2\n",
    "\n",
    "\n",
    "def delta_inv_im_real(z_train, poles, Y_train = ()):\n",
    "    Delta = np.zeros(len(poles) , dtype=complex )\n",
    "    for pole in poles:\n",
    "        Delta[p] = 1/np.real(pole) + 1/np.imag(pole)\n",
    "    return Delta \n",
    "\n",
    "\n",
    "def delta_inv_im_real(z_train, poles, Y_train = ()):\n",
    "    value = delta_inv_im_real(z_train, poles, Y_train)\n",
    "    return value**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMM_pole_filtering(delta_function,z_train, Y_train, poles , λ=0 , μ=0, num_PFBS_iter = 2000 , ε = 1.0e-8):\n",
    "    ## build the LS vectors and matrix\n",
    "    number_poles = poles.size\n",
    "    dim_residues = Y_train[0].size\n",
    "    Y_LS_vector = vectorize_Y_for_LS(z_train, Y_train, dim_residues)\n",
    "    Z = build_LS_matrix(z_train, Y_train, poles)\n",
    "    ## calculate the least distances from z_k to poles:\n",
    "    Δ = delta_function(z_train, poles, Y_train)\n",
    "    ## calculate the γ step size\n",
    "    ZZ = np.tensordot(Z.conj().T,Z,1)\n",
    "    eigenvals , eigenvects = np.linalg.eig(ZZ)\n",
    "    γ = 1/(2*np.amax(eigenvals))\n",
    "    ## condition number of this system\n",
    "    Condition_number = np.amax(eigenvals)/np.amin(eigenvals)\n",
    "    ## initiatilze the PFBS descent with the LS residues\n",
    "    LS_vector, LS_residual, LS_rank , LS_singular_values = np.linalg.lstsq(Z, Y_LS_vector)\n",
    "    PFBS_vector = LS_vector\n",
    "    ## start PFBS iterations\n",
    "    PFBS_iter_num = 0\n",
    "    for i in range(num_PFBS_iter):\n",
    "        ## report the old vector\n",
    "        PFBS_old_residues, PFBS_old_offset = extract_from_LS_vector(PFBS_vector,number_poles, dim_residues)\n",
    "        ## iteration count\n",
    "        PFBS_iter_num += 1 \n",
    "        ## compute the Gradient\n",
    "        ΔE = 2*np.tensordot(Z.conj().T, (np.tensordot(Z,PFBS_vector, 1) - Y_LS_vector), 1)\n",
    "        ## Add the Tichonov regularization on the residues\n",
    "        ΔEμ = ΔE \n",
    "        ΔEμ[:dim_residues*number_poles] = ΔE[:dim_residues*number_poles] + 2*μ*PFBS_vector[:dim_residues*number_poles]\n",
    "        ## take the Gradient descent step\n",
    "        GD_new_PFBS_vector = PFBS_vector - γ*ΔEμ\n",
    "        GD_new_residues, GD_new_offset = extract_from_LS_vector(GD_new_PFBS_vector, number_poles, dim_residues)\n",
    "        ## take the PFBS step for group LASSO regularization\n",
    "        PFBS_new_residues = GD_new_residues\n",
    "        ΔL2_relative_step_size = np.zeros([number_poles])\n",
    "        for p in range(number_poles):\n",
    "            PFBS_new_residues[p] = prox_λ(GD_new_residues[p],λ/Δ[p])\n",
    "            ## calculate the PFBS step size for convergence criteria\n",
    "            if np.linalg.norm(PFBS_new_residues[p]) == 0:\n",
    "                ΔL2_relative_step_size[p] = 0\n",
    "            else: \n",
    "                ΔL2_relative_step_size[p] = np.linalg.norm(PFBS_new_residues[p] - PFBS_old_residues[p])/(np.linalg.norm(PFBS_new_residues[p]))\n",
    "        ## update PFBS_vector\n",
    "        PFBS_vector = build_LS_vector(PFBS_new_residues, GD_new_offset)\n",
    "        ## convergence criteria on the relative step size \n",
    "        if np.amax(ΔL2_relative_step_size) < ε:\n",
    "            print(\"The λ regularization parameter is λ =\", λ, \"the maximum relative step sizes in norm is max(ΔL2_relative_step_size) =\" , np.amax(ΔL2_relative_step_size), \"for threshold ε =\", ε, \"and the PFBS iterations are breaking after PFBS_iter_num =\", PFBS_iter_num, \"iterations\")\n",
    "            break\n",
    "    PFBS_residues, PFBS_offset = extract_from_LS_vector(PFBS_vector, number_poles, dim_residues)\n",
    "    return PFBS_residues, PFBS_offset , γ , Condition_number, PFBS_iter_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nbFTl6EbPNY"
   },
   "source": [
    "#### Results of the Pole Filtering Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfUBHxbMbSI4"
   },
   "source": [
    "Pole Filtering trumps L1-L2 group regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizing_functions = [delta_min_distance, delta_min_distance_sq,delta_average_pole,delta_average_sq_pole,delta_average_pole_sq,delta_learning_rate,delta_learning_rate_sq,delta_inv_im_real]\n",
    "lambdas = [1e-9, 1e-12, 5*1e-8, 1e-8, 1e-8,1e-8,1e-8,1e-8]\n",
    "Y_output = []\n",
    "for jj, regularizer in enumerate(regularizing_functions):\n",
    "    RMM_residues, RMM_offset , γ , RMM_Condition_number, RMM_iter_num = RMM_pole_filtering(delta_min_distance,z_train, Y_train, VF_poles , lambdas[jj], 0.0)\n",
    "    ## Pole filtered results\n",
    "    z_train  ## for complex values : 2*np.random.rand(number_CV_points)*np.exp(1j*2*np.pi*np.random.rand(number_CV_points))\n",
    "    dim_residues = Y_train[0].size\n",
    "    Y_RMM = np.zeros([z_train.size, dim_residues] , dtype=complex) \n",
    "    for k in range(z_train.size):\n",
    "        Y_RMM[k] = rational_function(z_train[k], VF_poles, RMM_residues, RMM_offset )\n",
    "    Y_output.append(Y_RMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4010
    },
    "colab_type": "code",
    "id": "VySpx-LWh0BF",
    "outputId": "2a34e033-7195-46e5-eeb0-bfab6268d548",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plotting PFBS results v/s VF results v/s the true model data (log-space data is in absolute value)\n",
    "labels = [\"Y_PF\", \"Y_PF2\",  \"Y_Rational\", \"Y_2_Rational\",\"Y_Rational2\", \"Y_RMM\", \"Y_RMM2\",\"Y_inv\"]\n",
    "plt.figure()\n",
    "plot_title = 'Different regularizations'\n",
    "plt.plot(z_train, Y_train, 'x k', label='Y_train' )\n",
    "plt.plot(z_train, Y_VF, 'b', label='Y_VF' )\n",
    "#plt.plot(z_train, Y_PFBS, 'g', label='Y_PFBS')\n",
    "for ii, label in enumerate(labels):\n",
    "    plt.plot(z_train, Y_output[ii],label = label)\n",
    "plt.plot(z_train, Y_true, 'r', label='Y_true' )\n",
    "#plt.ylim(-10, 20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('F(z)')\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "pdf_title = \"%s.%s\"%(plot_title , 'pdf')\n",
    "plt.savefig(pdf_title)\n",
    "\n",
    "plt.figure()\n",
    "plot_title = 'Different regularizations (semilog)'\n",
    "plt.semilogy(z_train, np.abs(Y_train), 'x k', label='Y_train' )\n",
    "plt.semilogy(z_train, np.abs(Y_VF), 'b', label='Y_VF' )\n",
    "#plt.semilogy(z_train, np.abs(Y_PFBS), 'g', label='Y_PFBS')\n",
    "for ii, label in enumerate(labels):\n",
    "    plt.semilogy(z_train, Y_output[ii],label = label)\n",
    "plt.semilogy(z_train, np.abs(Y_true), 'r', label='Y_true' )\n",
    "#plt.ylim(-10, 20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('|F(z)| (log scale)')\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "pdf_title = \"%s.%s\"%(plot_title , 'pdf')\n",
    "plt.savefig(pdf_title)\n",
    "\n",
    "plt.figure()\n",
    "plot_title = 'Relative difference to true'\n",
    "plt.semilogy(z_train, np.abs((Y_true - Y_VF)/Y_true), 'b', label='Y_VF' )\n",
    "#plt.semilogy(z_train, np.abs((Y_true - Y_PFBS)/Y_true), 'g', label='Y_PFBS')\n",
    "for ii, label in enumerate(labels):\n",
    "    plt.semilogy(z_train, np.abs((Y_true - Y_output[ii])/Y_true),label = label)\n",
    "#plt.ylim(-10, 20)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('|Y_true(z_k) - F(z_k)|/|Y_true(z_k)|')\n",
    "plt.title(plot_title)\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "pdf_title = \"%s.%s\"%(plot_title , 'pdf')\n",
    "plt.savefig(pdf_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining (to be done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 3 things remaining at this stage: \n",
    "> • benchmark the VF alroghtm: in particular its ability to catch all the right poles if provided with (noisy) data. Because in the end we are supposing that VF would if given the right amount of poles. What if this is not the case?\n",
    "\n",
    "> • Test which pole filtering regularization works best.\n",
    "\n",
    "> • Test pole v/s residues regularization for final result with the right nuber of poles. \n",
    "\n",
    "> • It is likely that all the results depend on the form of the noise. Choose the right noise. \n",
    "\n",
    "> • Test RMM v/s brute force VF on time and performance. \n",
    "\n",
    "> • Implement and test the validity of the adaptive learning rates $\\gamma_n = \\frac{1}{2\\sum_k \\frac{\\rho_k}{|z_k - p_n|^2}}$\n",
    " \n",
    " > • Propose, implement, and test relationship between noise and lambda $\\lambda \\sim \\sigma$ with $\\sigma \\sim \\sum_k \\rho_k \\left\\| F(z_k) - Y_k \\right\\|_2^2$ \n",
    " \n",
    " > • Test of real-case problem: oxygen-16 (good example because ENDF does not give the resonance parameters, so we can only \"blindly\" fit the date, and observe wether we find what R-matrix theory finds. If these are neutral-particle chanels, the fundamental assumption that we are searching for rational functions is valid, and then we are indeed testing the poerformance of the Rational Matrix Machine on noisy date for a real case. If it finds the exact solution, we could claim we have a \"New Machine Learning Algorithm Learns Nuclear Physics\", which is sexy enought, certainly for ICML, but if the algorithm is very good, why not in Nature Communciations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection | Residues finding\n",
    "Comparing the performance of Tichonov regularization, residues dampening regularization (with squares?), and direct least-squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BJJCWgGTsnVq",
    "MMk2S0oURs9D",
    "nTIvzAVUNmg3",
    "O__W6aVJN3_O",
    "0nbFTl6EbPNY"
   ],
   "name": "Copy of Rational Matrix Machines.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
